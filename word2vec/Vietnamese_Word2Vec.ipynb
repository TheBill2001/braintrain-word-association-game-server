{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyM_xy5hkAsI"
      },
      "source": [
        "# Build a Vietnamese Text Corpus from Wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2tyFDMKqd0E"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pag-bujqd0F"
      },
      "source": [
        "### Installing dependencies\n",
        "- `gensim`: For training Word2Vec.\n",
        "- `underthesea`: For data processing.\n",
        "- `Cython`: Required by `gensim`, may not be explicitly installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coUxx1KJj-9b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install gensim underthesea Cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M02M__gmqd0H"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from underthesea import text_normalize, word_tokenize\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "from gensim.corpora import WikiCorpus\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models.callbacks import CallbackAny2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHywTsO3qd0H"
      },
      "source": [
        "### Download Wikidump"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7u0DzSQkAR9"
      },
      "outputs": [],
      "source": [
        "TEXT_DUMP = \"https://dumps.wikimedia.org/viwiki/20240120/viwiki-20240120-pages-articles.xml.bz2\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_GqpyNkqd0J"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/content/data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqsAK_jgqd0K"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $DATA_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbxeTzi9qd0L"
      },
      "outputs": [],
      "source": [
        "WIKIDUMP_FILE = os.path.join(DATA_DIR, \"wikidump.bz2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3Mvh6Pml_HM"
      },
      "outputs": [],
      "source": [
        "!curl -L -o $WIKIDUMP_FILE -C - $TEXT_DUMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXsTqdSBqd0L"
      },
      "source": [
        "### Download the dictionary\n",
        "\n",
        "The dictionaries are provided from two main sources:\n",
        "\n",
        "- Vietnamese Hunspell library: The open sourced spell checking library, Vietnamese language version.\n",
        "- UTS Dictionary: By Underthesea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkfSHpm_qd0M"
      },
      "outputs": [],
      "source": [
        "DICTIONARY_FILE = os.path.join(DATA_DIR, \"dictionary.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FBXp-UWqd0M",
        "outputId": "c4b01282-7254-423e-a090-e88f5b417ff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 39910  100 39910    0     0  85433      0 --:--:-- --:--:-- --:--:-- 85460\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 39916  100 39916    0     0  80335      0 --:--:-- --:--:-- --:--:-- 80313\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1174  100  1174    0     0   3210      0 --:--:-- --:--:-- --:--:--  3216\n",
            "100  952k  100  952k    0     0  1458k      0 --:--:-- --:--:-- --:--:-- 11.3M\n"
          ]
        }
      ],
      "source": [
        "!curl -L \"https://raw.githubusercontent.com/1ec5/hunspell-vi/master/dictionaries/vi-DauCu.dic\" > $DICTIONARY_FILE\n",
        "!curl -L \"https://raw.githubusercontent.com/1ec5/hunspell-vi/master/dictionaries/vi-DauMoi.dic\" >> $DICTIONARY_FILE\n",
        "!curl -L \"https://huggingface.co/datasets/undertheseanlp/UTS_Dictionary/resolve/main/data/data.txt?download=true\" >> $DICTIONARY_FILE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wompgxzJo-TK"
      },
      "source": [
        "## Making the corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkZ5NJzsvtwb"
      },
      "source": [
        "### Building vocab\n",
        "\n",
        "From the dictionaries, we build a vocab set. To simplify the vocab set, we will only use vocab with two words or fewer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApCvmqVoqd0N"
      },
      "outputs": [],
      "source": [
        "def make_vocab():\n",
        "    vocab = set()\n",
        "    with open(DICTIONARY_FILE, \"r\") as f:\n",
        "        for line in f:\n",
        "            if len(line.split()) <= 2:\n",
        "                vocab.add(text_normalize(line).lower())\n",
        "    return vocab\n",
        "\n",
        "\n",
        "VOCAB = make_vocab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCeB_G5Bqd0N",
        "outputId": "b4ab53b9-f8ec-4d1c-ba10-22392c407c3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab count: 58070\n",
            "['sacarin', 'gà vịt', 'khổng tử', 'tốt vía', 'kết dính', 'bổ khí', 'aceton', 'mượt mà', 'lãnh dục', 'giảm biên', 'hoảng', 'làm rẫy', 'uẩn khúc', 'kem', 'công kênh', 'cuối tuần', 'váy xòe', 'nghỉu', 'thiêu thân', 'tín điều']\n"
          ]
        }
      ],
      "source": [
        "print(\"Vocab count: {}\".format(len(VOCAB)))\n",
        "print(list(VOCAB)[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyIxbgbgqd0O"
      },
      "source": [
        "### Define the tokenizer function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPzBXUrqqd0O"
      },
      "outputs": [],
      "source": [
        "def tokenizer_func(text: str, token_min_len: int, token_max_len: int, lower: bool):\n",
        "    tokens = list()\n",
        "    for token in word_tokenize(text):\n",
        "        if not (token_min_len <= len(token) <= token_max_len):\n",
        "            continue\n",
        "        if lower:\n",
        "            token = token.lower()\n",
        "        if token in VOCAB:\n",
        "            tokens.append(token)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L3_8uj5qd0O"
      },
      "source": [
        "### Building the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM52aMVpqd0P"
      },
      "outputs": [],
      "source": [
        "CORPUS_FILE = os.path.join(DATA_DIR, \"vi-wiki-corpus-token.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71w2RDiLqd0P"
      },
      "outputs": [],
      "source": [
        "corpus = WikiCorpus(WIKIDUMP_FILE, tokenizer_func=tokenizer_func, dictionary={}, lower=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oos7qvvjYFmp",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def make_corpus():\n",
        "    with open(CORPUS_FILE, \"wb\") as f:\n",
        "        print(\"Tokenizing corpus...\")\n",
        "\n",
        "        for index, text in enumerate(corpus.get_texts()):\n",
        "            if (index % 1000 == 0):\n",
        "                print('Processed {} articles'.format(index))\n",
        "            pickle.dump(text, f)\n",
        "\n",
        "        print('Processing complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "4tbqqI1gqd0P"
      },
      "outputs": [],
      "source": [
        "OVERRIDE = False\n",
        "\n",
        "if not os.path.isfile(CORPUS_FILE):\n",
        "    make_corpus()\n",
        "else:\n",
        "    if OVERRIDE:\n",
        "        make_corpus()\n",
        "    else:\n",
        "        print(\"Corpus existed...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUP5sG-3qd0P"
      },
      "source": [
        "### Preview corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "1ArlY72vqd0P",
        "outputId": "e98e68b2-05b9-4b7c-87b4-1463f4d51b1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 line(s) of corpus\n",
            "['tiếng', 'việt', 'cũng', 'gọi là', 'tiếng', 'hay', 'việt ngữ', 'là', 'ngôn ngữ', 'của'] ...\n",
            "['còn', 'được', 'người', 'việt', 'gọi', 'vắn tắt', 'là', 'hay', 'phiên', 'là'] ...\n",
            "['thành phố', 'viết', 'tắt', 'hay', 'sài gòn', 'là', 'thành phố', 'lớn', 'nhất', 'và'] ...\n",
            "['là', 'tổ chức', 'tiêu chuẩn', 'quốc tế', 'chính', 'cho', 'được', 'thành lập', 'vào', 'năm'] ...\n",
            "['lào', 'tên', 'chính thức', 'là', 'cộng hòa', 'lào', 'là', 'quốc gia', 'có', 'chủ quyền'] ...\n"
          ]
        }
      ],
      "source": [
        "if not os.path.isfile(CORPUS_FILE):\n",
        "    print(\"Cannot find corpus\")\n",
        "else:\n",
        "    with open(CORPUS_FILE, \"rb\") as f:\n",
        "        count = 0\n",
        "        read_limit = 5\n",
        "        print(\"First {} line(s) of corpus\".format(read_limit))\n",
        "        while count < 5:\n",
        "            count = count + 1\n",
        "            try:\n",
        "                data = pickle.load(f)\n",
        "                if len(data) > 10:\n",
        "                    print(\"{} ...\".format(pickle.load(f)[:10]))\n",
        "                else:\n",
        "                    print(data)\n",
        "            except EOFError:\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrKDMZPqmShK"
      },
      "source": [
        "## Build Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2o5gjtlqd0Q"
      },
      "source": [
        "`MAX_SENTENCE` denotes the number of sentences will be processed. Set `MAX_SENTENCE` to `-1` to use all corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F2T6rt75mWBM"
      },
      "outputs": [],
      "source": [
        "MAX_SENTENCE = -1 # @param {type: \"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YvyAa96tIau"
      },
      "outputs": [],
      "source": [
        "class MySentences(object):\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        with open(CORPUS_FILE, \"rb\") as f:\n",
        "            while self.count < MAX_SENTENCE or MAX_SENTENCE == -1:\n",
        "                self.count = self.count + 1\n",
        "                try:\n",
        "                    yield pickle.load(f)\n",
        "                except EOFError:\n",
        "                    break\n",
        "\n",
        "\n",
        "class MyCallbacks(CallbackAny2Vec):\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        print(\"Epoch: \" + str(self.epoch) +  \", Loss: \" + str(model.get_latest_training_loss()))\n",
        "        model.running_training_loss = 0.0\n",
        "        self.epoch += 1\n",
        "\n",
        "\n",
        "callbacks = [MyCallbacks()]\n",
        "sentences = MySentences()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3SIkMxRqd0R"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIR = \"/content/out\"\n",
        "!mkdir -p $OUTPUT_DIR\n",
        "\n",
        "MODEL_PATH = os.path.join(OUTPUT_DIR, \"vi-word2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gIDPzo0ctGva",
        "outputId": "de3f8c0b-8c73-4d4c-bd43-b23c1c540695",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 94236304.0\n",
            "Epoch: 1, Loss: 93933872.0\n",
            "Epoch: 2, Loss: 93315536.0\n",
            "Epoch: 3, Loss: 92566296.0\n",
            "Epoch: 4, Loss: 92068112.0\n",
            "Epoch: 5, Loss: 91679168.0\n",
            "Epoch: 6, Loss: 91088648.0\n",
            "Epoch: 7, Loss: 90658784.0\n",
            "Epoch: 8, Loss: 90247440.0\n",
            "Epoch: 9, Loss: 89908392.0\n",
            "Epoch: 10, Loss: 89519416.0\n",
            "Epoch: 11, Loss: 89144616.0\n",
            "Epoch: 12, Loss: 88493720.0\n",
            "Epoch: 13, Loss: 87958984.0\n",
            "Epoch: 14, Loss: 87538344.0\n",
            "Epoch: 15, Loss: 87411360.0\n",
            "Epoch: 16, Loss: 86728184.0\n",
            "Epoch: 17, Loss: 86242384.0\n",
            "Epoch: 18, Loss: 85243752.0\n",
            "Epoch: 19, Loss: 83609160.0\n"
          ]
        }
      ],
      "source": [
        "model = Word2Vec(sentences, workers=12, sample=1e-3, min_count=10,\n",
        "                 vector_size=100, window=15, sg=1,\n",
        "                 epochs=20, compute_loss=True, callbacks=callbacks)\n",
        "model.save(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA0WNkl2qd0S"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec.load(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzTHdG-5AK2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5175ef35-9b32-4f2b-c19f-b828f5c52d07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bản chất', 0.8088125586509705),\n",
              " ('tâm trí', 0.7885621786117554),\n",
              " ('khía cạnh', 0.787632167339325),\n",
              " ('tinh thần', 0.784379780292511),\n",
              " ('tưởng tượng', 0.7830929160118103),\n",
              " ('nhận thức', 0.7775944471359253),\n",
              " ('đạo đức', 0.77630615234375),\n",
              " ('loài người', 0.7752376198768616),\n",
              " ('siêu nhiên', 0.7739694118499756),\n",
              " ('ý thức', 0.7722036242485046)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "model.wv.most_similar([\"hành động\", \"con người\"], topn=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWLcjN9Pqd0S"
      },
      "source": [
        "# Export vectors and metadata\n",
        "\n",
        "- Can be viewed using [embedding projector](https://projector.tensorflow.org/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydk2y3nCqd0S"
      },
      "outputs": [],
      "source": [
        "METADATA_PATH = os.path.join(OUTPUT_DIR, \"metadata.tsv\")\n",
        "VECTOR_PATH = os.path.join(OUTPUT_DIR, \"vectors.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "1OngHtVYqd0T"
      },
      "outputs": [],
      "source": [
        "keys = model.wv.index_to_key\n",
        "\n",
        "with open(METADATA_PATH, \"w\", encoding=\"utf-8\") as metadata:\n",
        "    for key in keys:\n",
        "        metadata.writelines(key + \"\\n\")\n",
        "\n",
        "with open(VECTOR_PATH, \"w\", encoding=\"utf-8\") as vectors:\n",
        "    for key in keys:\n",
        "        # Normalize vector `norm=True`\n",
        "        vector = \"\\t\".join([str(v) for v in model.wv.get_vector(key, norm=True).tolist()])\n",
        "        vectors.write(vector + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select start words"
      ],
      "metadata": {
        "id": "EWwIFhgKdzNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORY = [ # The categories we want to extract\n",
        "    [\"gia đình\"],\n",
        "    [\"việc làm\"],\n",
        "    [\"động vật\", \"chó\"],\n",
        "    [\"đồ ăn\"],\n",
        "    [\"tính cách\"]\n",
        "]\n",
        "NO_WORDS = 10 # Number of words per category\n",
        "\n",
        "words_list = list()\n",
        "for cat in CATEGORY:\n",
        "  words = [word[0] for word in model.wv.most_similar(cat, topn=NO_WORDS)]\n",
        "  words_list.append({\"category\": cat, \"words\": words})\n",
        "\n",
        "words_list_json = json.dumps(words_list, indent=4, ensure_ascii=False)\n",
        "print(words_list_json)\n",
        "with open(os.path.join(OUTPUT_DIR, \"categories.json\"), \"w\", encoding='utf8') as outfile:\n",
        "    outfile.write(words_list_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZi25vW0d6wz",
        "outputId": "5d2ba896-e40c-4b28-8a77-f2c348234985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"category\": [\n",
            "            \"gia đình\"\n",
            "        ],\n",
            "        \"words\": [\n",
            "            \"cha mẹ\",\n",
            "            \"cha\",\n",
            "            \"ông bà\",\n",
            "            \"mẹ\",\n",
            "            \"chị gái\",\n",
            "            \"cha dượng\",\n",
            "            \"vợ\",\n",
            "            \"bà\",\n",
            "            \"người thân\",\n",
            "            \"bố mẹ\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"category\": [\n",
            "            \"việc làm\"\n",
            "        ],\n",
            "        \"words\": [\n",
            "            \"lao động\",\n",
            "            \"khuyến khích\",\n",
            "            \"tuyển dụng\",\n",
            "            \"lợi ích\",\n",
            "            \"thất nghiệp\",\n",
            "            \"công bằng\",\n",
            "            \"phúc lợi\",\n",
            "            \"trợ cấp\",\n",
            "            \"nghề nghiệp\",\n",
            "            \"thúc đẩy\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"category\": [\n",
            "            \"động vật\",\n",
            "            \"chó\"\n",
            "        ],\n",
            "        \"words\": [\n",
            "            \"thú\",\n",
            "            \"mèo\",\n",
            "            \"gấu\",\n",
            "            \"thỏ\",\n",
            "            \"gấu mèo\",\n",
            "            \"khỉ\",\n",
            "            \"hươu\",\n",
            "            \"săn\",\n",
            "            \"sói\",\n",
            "            \"lửng\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"category\": [\n",
            "            \"đồ ăn\"\n",
            "        ],\n",
            "        \"words\": [\n",
            "            \"thực đơn\",\n",
            "            \"ăn\",\n",
            "            \"ăn vặt\",\n",
            "            \"thực khách\",\n",
            "            \"nhà hàng\",\n",
            "            \"món\",\n",
            "            \"bữa\",\n",
            "            \"ăn liền\",\n",
            "            \"ăn ở\",\n",
            "            \"nấu nướng\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"category\": [\n",
            "            \"tính cách\"\n",
            "        ],\n",
            "        \"words\": [\n",
            "            \"ngây thơ\",\n",
            "            \"trầm tính\",\n",
            "            \"lạnh lùng\",\n",
            "            \"bướng bỉnh\",\n",
            "            \"tốt bụng\",\n",
            "            \"cá tính\",\n",
            "            \"vụng về\",\n",
            "            \"rụt rè\",\n",
            "            \"nóng nảy\",\n",
            "            \"hoạt bát\"\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}